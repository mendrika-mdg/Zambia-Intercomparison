{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d7b376",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe8e4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np      \n",
    "from netCDF4 import Dataset  \n",
    "from scipy.ndimage import label\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.insert(1, \"/home/users/mendrika/SSA/SA/module\")\n",
    "import snflics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc0a07",
   "metadata": {},
   "source": [
    "# These are specific to Zambia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e679d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = 547, 970\n",
    "x_min, x_max = 1436, 1898\n",
    "\n",
    "CONTEXT_LAT_MIN = -19\n",
    "CONTEXT_LAT_MAX = -7\n",
    "CONTEXT_LON_MIN = 21\n",
    "CONTEXT_LON_MAX = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6be61",
   "metadata": {},
   "source": [
    "# Crop geo coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea96ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = np.load(\"/gws/nopw/j04/cocoon/SSA_domain/lat_lon_2268_2080.npz\")\n",
    "lons = geodata[\"lon\"][y_min:y_max+1, x_min:x_max+1]\n",
    "lats = geodata[\"lat\"][y_min:y_max+1, x_min:x_max+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a37ab",
   "metadata": {},
   "source": [
    "## Read core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e63b6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_core(file):\n",
    "\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"The file '{file}' does not exist.\")\n",
    "    try:\n",
    "        # using a context manager to ensure proper file closure\n",
    "        with Dataset(file, \"r\") as data:\n",
    "            cores = data.variables[\"cores\"][0, y_min:y_max+1, x_min:x_max+1]\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error opening NetCDF file: {file}. {e}\")\n",
    "\n",
    "    return cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f59d3",
   "metadata": {},
   "source": [
    "## Move a datetime by any given dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2548a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hour(date_dict, hours_to_add, minutes_to_add):\n",
    "    \"\"\"\n",
    "    Add hours and minutes to a datetime dictionary and return the updated dict and a generated file path.\n",
    "\n",
    "    Args:\n",
    "        date_dict     (dict): Keys: 'year', 'month', 'day', 'hour', 'minute' as strings, e.g. \"01\", \"23\"\n",
    "        hours_to_add   (int): Number of hours to add.\n",
    "        minutes_to_add (int): Number of minutes to add.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - dict: Updated datetime dictionary with all fields as zero-padded strings.\n",
    "            - str: File path in the format YYYY/MM/YYYYMMDDHHMM.nc\n",
    "    \"\"\"\n",
    "    # Parse the original time\n",
    "    time_obj = datetime(\n",
    "        int(date_dict[\"year\"]),\n",
    "        int(date_dict[\"month\"]),\n",
    "        int(date_dict[\"day\"]),\n",
    "        int(date_dict[\"hour\"]),\n",
    "        int(date_dict[\"minute\"])\n",
    "    )\n",
    "\n",
    "    # Add hours\n",
    "    updated = time_obj + timedelta(hours=hours_to_add, minutes=minutes_to_add)\n",
    "\n",
    "    # Format updated dictionary\n",
    "    new_date_dict = {\n",
    "        \"year\":   f\"{updated.year:04d}\",\n",
    "        \"month\":  f\"{updated.month:02d}\",\n",
    "        \"day\":    f\"{updated.day:02d}\",\n",
    "        \"hour\":   f\"{updated.hour:02d}\",\n",
    "        \"minute\": f\"{updated.minute:02d}\"\n",
    "    }\n",
    "\n",
    "    # Generate file path\n",
    "    file_path = f\"{new_date_dict['year']}/{new_date_dict['month']}/{new_date_dict['year']}{new_date_dict['month']}{new_date_dict['day']}{new_date_dict['hour']}{new_date_dict['minute']}.nc\"\n",
    "\n",
    "\n",
    "    return {'time': new_date_dict, 'path': file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300d668",
   "metadata": {},
   "source": [
    "## Extract a box of given size for y,x coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67e3a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_box(matrix, y, x, box_size=3):\n",
    "    half = box_size // 2\n",
    "    y_min = max(y - half, 0)\n",
    "    y_max = min(y + half + 1, matrix.shape[0])\n",
    "    x_min = max(x - half, 0)\n",
    "    x_max = min(x + half + 1, matrix.shape[1])\n",
    "    return matrix[y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba877b00",
   "metadata": {},
   "source": [
    "## Creating storm databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d43a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storm_database(data_t, lats, lons):\n",
    "    \"\"\"\n",
    "    Identify storm cores and extract features for each core.\n",
    "\n",
    "    Args:\n",
    "        data_t (Dataset): Dataset containing 'cores' and 'tir' variables.\n",
    "        lats, lons (np.ndarray): 2D lat/lon arrays of the domain.\n",
    "\n",
    "    Returns:\n",
    "        dict: Storm database indexed by core label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crop domain\n",
    "    cores_t = data_t[\"cores\"][0, y_min:y_max+1, x_min:x_max+1]\n",
    "    tir_t   = data_t[\"tir\"][0, y_min:y_max+1, x_min:x_max+1]\n",
    "\n",
    "    # Max lat/lon of all detected power maxima\n",
    "    Pmax_lat, Pmax_lon = data_t[\"max_lat\"][:], data_t[\"max_lon\"][:]\n",
    "\n",
    "    # Restrict to context window\n",
    "    valid = (\n",
    "        (Pmax_lon >= CONTEXT_LON_MIN) & (Pmax_lon <= CONTEXT_LON_MAX) &\n",
    "        (Pmax_lat >= CONTEXT_LAT_MIN) & (Pmax_lat <= CONTEXT_LAT_MAX)\n",
    "    )\n",
    "    Pmax_lat, Pmax_lon = Pmax_lat[valid], Pmax_lon[valid]\n",
    "\n",
    "    # Label connected components\n",
    "    labeled_array, _ = label(cores_t != 0)\n",
    "    core_labels = np.unique(labeled_array[labeled_array != 0])\n",
    "\n",
    "    # Core-wise properties\n",
    "    dict_storm_size      = {lab: np.sum(labeled_array == lab) * 9 for lab in core_labels}\n",
    "\n",
    "    dict_storm_extent = {}\n",
    "    for lab in core_labels:\n",
    "        mask = labeled_array == lab\n",
    "        dict_storm_extent[lab] = {\n",
    "            \"lat_min\": float(np.nanmin(lats[mask])),\n",
    "            \"lat_max\": float(np.nanmax(lats[mask])),\n",
    "            \"lon_min\": float(np.nanmin(lons[mask])),\n",
    "            \"lon_max\": float(np.nanmax(lons[mask]))\n",
    "        }\n",
    "\n",
    "    # Minimum TIR (3×3 mean around coldest pixel)\n",
    "    dict_storm_temperature = {}\n",
    "    for lab in core_labels:\n",
    "        mask = labeled_array == lab\n",
    "        tir_core = tir_t[mask]\n",
    "        yx_indices = np.argwhere(mask)\n",
    "        y, x = yx_indices[np.argmin(tir_core)]\n",
    "        box = extract_box(tir_t, y, x)\n",
    "        dict_storm_temperature[lab] = float(np.mean(box))\n",
    "\n",
    "    # Assemble final database\n",
    "    storm_database = {}\n",
    "    for lat, lon in zip(Pmax_lat, Pmax_lon):\n",
    "        try:\n",
    "            y, x = snflics.to_yx(lat, lon, lats, lons)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        lab = labeled_array[y, x]\n",
    "        if lab == 0 or lab in storm_database:\n",
    "            continue\n",
    "\n",
    "        ext = dict_storm_extent[lab]\n",
    "        storm_database[int(lab)] = {\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "            \"lat_min\": ext[\"lat_min\"],\n",
    "            \"lat_max\": ext[\"lat_max\"],\n",
    "            \"lon_min\": ext[\"lon_min\"],\n",
    "            \"lon_max\": ext[\"lon_max\"],\n",
    "            \"tir\": dict_storm_temperature[lab],\n",
    "            \"size\": dict_storm_size[lab],\n",
    "            \"mask\": 1,\n",
    "        }\n",
    "    return storm_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47bca889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fictional_storm(context_lat_min, context_lat_max,\n",
    "                             context_lon_min, context_lon_max):\n",
    "    \"\"\"\n",
    "    Generate a dummy (non-convective) storm entry with mask=0.\n",
    "    Used when padding cores — values don't affect the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (storm_id, storm_dict)\n",
    "    \"\"\"\n",
    "    # Pick a random coordinate in or near the domain (anywhere is fine)\n",
    "    lat = np.random.uniform(context_lat_min, context_lat_max)\n",
    "    lon = np.random.uniform(context_lon_min, context_lon_max)\n",
    "\n",
    "    # Create consistent placeholder values\n",
    "    storm = {\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "        \"lat_min\": lat,\n",
    "        \"lat_max\": lat,\n",
    "        \"lon_min\": lon,\n",
    "        \"lon_max\": lon,\n",
    "        \"tir\": 30.0,     # warm non-convective background\n",
    "        \"size\": 0.0,     \n",
    "        \"mask\": 0        # ensures Transformer ignores it\n",
    "    }\n",
    "\n",
    "    return (\"artificial\", storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67a0775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_observed_storms(storm_db, nb_x0,\n",
    "                        context_lat_min, context_lat_max,\n",
    "                        context_lon_min, context_lon_max):\n",
    "    \"\"\"\n",
    "    Ensure a fixed number of storm cores by either truncating or padding.\n",
    "\n",
    "    Args:\n",
    "        storm_db (dict): Dictionary of observed storms {id: storm_dict}.\n",
    "        nb_x0 (int): Target number of cores expected by the model.\n",
    "        context_lat_min, context_lat_max, context_lon_min, context_lon_max (float): \n",
    "            Domain boundaries.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (storm_id, storm_dict) tuples of length nb_x0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert dict to list\n",
    "    storm_list = list(storm_db.items())\n",
    "\n",
    "    # --- CASE 1: too many cores, keep the strongest (coldest) ones ---\n",
    "    if len(storm_list) >= nb_x0:\n",
    "        # Sort by TIR ascending (colder = stronger convection)\n",
    "        sorted_db = sorted(storm_list, key=lambda item: item[1]['tir'])\n",
    "        return sorted_db[:nb_x0]\n",
    "\n",
    "    # --- CASE 2: too few cores, pad with artificial storms ---\n",
    "    needed = nb_x0 - len(storm_list)\n",
    "    for _ in range(needed):\n",
    "        storm_list.append(\n",
    "            generate_fictional_storm(\n",
    "                context_lat_min=context_lat_min,\n",
    "                context_lat_max=context_lat_max,\n",
    "                context_lon_min=context_lon_min,\n",
    "                context_lon_max=context_lon_max\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return storm_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649fb57",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08e406aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_array(data, time_lag):\n",
    "    \"\"\"\n",
    "    Transform list of (id, dict) storm entries into a NumPy array.\n",
    "\n",
    "    Each row corresponds to one storm core at a given time lag.\n",
    "\n",
    "    Args:\n",
    "        time_obs (dict): Dictionary with keys ['year','month','day','hour','minute'].\n",
    "        data (list): List of (id, storm_dict) tuples from pad_observed_storms().\n",
    "        time_lag (int): Time lag index (e.g., 0, 1, 2).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, F) with per-core features.\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    for _, entry in data:\n",
    "        lat      = float(entry[\"lat\"])\n",
    "        lon      = float(entry[\"lon\"])\n",
    "        lat_min  = float(entry.get(\"lat_min\", lat))\n",
    "        lat_max  = float(entry.get(\"lat_max\", lat))\n",
    "        lon_min  = float(entry.get(\"lon_min\", lon))\n",
    "        lon_max  = float(entry.get(\"lon_max\", lon))\n",
    "        tir      = float(entry[\"tir\"])\n",
    "        size     = float(entry[\"size\"])\n",
    "        mask     = int(entry[\"mask\"])\n",
    "\n",
    "        # Define the per-core feature vector (local only)\n",
    "        # [lat, lon, lat_min, lat_max, lon_min, lon_max, tir, size, wp, mask, lag]\n",
    "        result.append([\n",
    "            lat, lon,\n",
    "            lat_min, lat_max,\n",
    "            lon_min, lon_max,\n",
    "            tir, size,\n",
    "            mask, time_lag\n",
    "        ])\n",
    "\n",
    "    return np.array(result, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e86bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_X0 = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9a60b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2024\"\n",
    "\n",
    "# Data where all the historical cores are located and the output folder\n",
    "DATA_PATH = \"/gws/nopw/j04/cocoon/SSA_domain/ch9_wavelet/\"\n",
    "OUTPUT_FOLDER = \"/gws/nopw/j04/wiser_ewsa/mrakotomanga/Intercomparison/raw\"\n",
    "\n",
    "# year of interest\n",
    "all_files = [file for file in snflics.all_files_in(DATA_PATH) if snflics.get_time(file)[\"year\"] == YEAR and snflics.get_time(file)[\"month\"] in [\"12\", \"01\", \"02\"]]\n",
    "all_files.sort()\n",
    "\n",
    "# Number of storms to consider (after analysing the whole dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "102075e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_t, nb_x0, time_lag,\n",
    "                 lats, lons,\n",
    "                 CONTEXT_LAT_MIN, CONTEXT_LAT_MAX,\n",
    "                 CONTEXT_LON_MIN, CONTEXT_LON_MAX):\n",
    "    \"\"\"\n",
    "    Process one NetCDF file and return the per-core input tensor for the model.\n",
    "\n",
    "    Args:\n",
    "        file_t (str): Path to NetCDF file at time t.\n",
    "        nb_x0 (int): Number of storm cores to keep/pad (e.g. 96).\n",
    "        time_lag (int): Lag index (0, 1, 2).\n",
    "        lats, lons (np.ndarray): 2D arrays for lat/lon of the domain grid.\n",
    "        CONTEXT_LAT_MIN/MAX, CONTEXT_LON_MIN/MAX (float): Domain boundaries.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (nb_x0, F) containing per-core features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Dataset(file_t, \"r\") as data_t:\n",
    "            # Read storm maxima\n",
    "            x0_lat = data_t[\"max_lat\"][:]\n",
    "            x0_lon = data_t[\"max_lon\"][:]\n",
    "\n",
    "            valid = (\n",
    "                (x0_lat >= CONTEXT_LAT_MIN) & (x0_lat <= CONTEXT_LAT_MAX) &\n",
    "                (x0_lon >= CONTEXT_LON_MIN) & (x0_lon <= CONTEXT_LON_MAX)\n",
    "            )\n",
    "\n",
    "            if np.sum(valid) == 0:\n",
    "                return None  # no storms in the domain\n",
    "\n",
    "            x0_lat = x0_lat[valid]\n",
    "            x0_lon = x0_lon[valid]\n",
    "\n",
    "            # Extract storm features for this file\n",
    "            storm_database = create_storm_database(data_t, lats, lons)\n",
    "\n",
    "            # Pad/truncate to nb_x0 cores\n",
    "            X_features = pad_observed_storms(\n",
    "                storm_database, nb_x0,\n",
    "                CONTEXT_LAT_MIN, CONTEXT_LAT_MAX,\n",
    "                CONTEXT_LON_MIN, CONTEXT_LON_MAX\n",
    "            )\n",
    "\n",
    "            # Convert to numpy array\n",
    "            input_features = transform_to_array(X_features, time_lag)\n",
    "\n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.tensor(input_features, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_t}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8557ad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5661"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39680678",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_X0 = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97ae0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved input tensor + global context: /gws/nopw/j04/wiser_ewsa/mrakotomanga/Intercomparison/raw/inputs_t0/input-20240102_0100.pt\n",
      "Saved 5 targets for 20240102_0100\n"
     ]
    }
   ],
   "source": [
    "for file_t in all_files[100:101]:\n",
    "\n",
    "    # Current nowcast origin time\n",
    "    time_t = snflics.get_time(file_t)\n",
    "\n",
    "    # Lag times (in minutes) before t\n",
    "    lag_before_t = [0, 60, 120]  # t0, t-1h, t-2h\n",
    "    file_before_t = [\n",
    "        DATA_PATH + update_hour(time_t, hours_to_add=0, minutes_to_add=-m)[\"path\"]\n",
    "        for m in lag_before_t\n",
    "    ]\n",
    "\n",
    "    # Lead times (in hours) after t\n",
    "    lead_times = [0, 1, 2, 4, 6]\n",
    "    file_lead_times = [\n",
    "        DATA_PATH + update_hour(time_t, hours_to_add=h, minutes_to_add=0)[\"path\"]\n",
    "        for h in lead_times\n",
    "    ]\n",
    "\n",
    "    # Time components\n",
    "    year   = int(time_t[\"year\"])\n",
    "    month  = int(time_t[\"month\"])\n",
    "    day    = int(time_t[\"day\"])\n",
    "    hour   = int(time_t[\"hour\"])\n",
    "    minute = int(time_t[\"minute\"])\n",
    "\n",
    "    # Output paths\n",
    "    NOWCAST_ORIGIN = f\"{year:04d}{month:02d}{day:02d}_{hour:02d}{minute:02d}\"\n",
    "    INPUT_LT0 = f\"{OUTPUT_FOLDER}/inputs_t0/input-{NOWCAST_ORIGIN}.pt\"\n",
    "    OUTPUT_PATHS = {\n",
    "        f\"LT{i}\": f\"{OUTPUT_FOLDER}/targets_t{i}/target-{NOWCAST_ORIGIN}.pt\"\n",
    "        for i in lead_times\n",
    "    }\n",
    "\n",
    "    # Check that all required files exist\n",
    "    if not (all(os.path.exists(f) for f in file_lead_times) and all(os.path.exists(f) for f in file_before_t)):\n",
    "        print(f\"Missing required files for {file_t}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Prepare targets (future lead-time cores)\n",
    "        core_series = [prepare_core(f) for f in file_lead_times]\n",
    "    except OSError:\n",
    "        print(f\"Skipping {file_t}: unreadable core file.\")\n",
    "        continue\n",
    "\n",
    "    # Check if there are valid cores at t0\n",
    "    with Dataset(file_t, \"r\") as data_t:\n",
    "        Pmax_lat = data_t[\"max_lat\"][:]\n",
    "        Pmax_lon = data_t[\"max_lon\"][:]\n",
    "\n",
    "        valid = (\n",
    "            (Pmax_lon >= CONTEXT_LON_MIN) & (Pmax_lon <= CONTEXT_LON_MAX) &\n",
    "            (Pmax_lat >= CONTEXT_LAT_MIN) & (Pmax_lat <= CONTEXT_LAT_MAX)\n",
    "        )\n",
    "        Pmax_lat, Pmax_lon = Pmax_lat[valid], Pmax_lon[valid]\n",
    "\n",
    "        if Pmax_lat.size == 0:\n",
    "            print(f\"No core in the domain for {NOWCAST_ORIGIN}\")\n",
    "            continue\n",
    "\n",
    "    # Process inputs for each lag (t0, t-1h, t-2h)\n",
    "    input_tensors = []\n",
    "    for i, f in enumerate(file_before_t):\n",
    "        t_tensor = process_file(\n",
    "            f,\n",
    "            nb_x0=NB_X0,\n",
    "            time_lag=lag_before_t[i],\n",
    "            lats=lats,\n",
    "            lons=lons,\n",
    "            CONTEXT_LAT_MIN=CONTEXT_LAT_MIN,\n",
    "            CONTEXT_LAT_MAX=CONTEXT_LAT_MAX,\n",
    "            CONTEXT_LON_MIN=CONTEXT_LON_MIN,\n",
    "            CONTEXT_LON_MAX=CONTEXT_LON_MAX\n",
    "        )\n",
    "        if t_tensor is not None:\n",
    "            input_tensors.append(t_tensor)\n",
    "\n",
    "    if not input_tensors:\n",
    "        print(f\"No valid input tensor for {NOWCAST_ORIGIN}\")\n",
    "        continue\n",
    "\n",
    "    # Concatenate all lag inputs into one tensor\n",
    "    input_tensor = torch.cat(input_tensors, dim=0)  # (288, F)\n",
    "\n",
    "    # Compute global context (month/time sin–cos)\n",
    "    month_angle = 2 * np.pi * (month - 1) / 12\n",
    "    tod_angle   = 2 * np.pi * (hour + minute / 60.0) / 24\n",
    "    global_context = torch.tensor([\n",
    "        np.sin(month_angle), np.cos(month_angle),\n",
    "        np.sin(tod_angle), np.cos(tod_angle)\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    # Save both input tensor and global context\n",
    "    torch.save({\n",
    "        \"input_tensor\": input_tensor,\n",
    "        \"global_context\": global_context,\n",
    "        \"nowcast_origin\": NOWCAST_ORIGIN\n",
    "    }, INPUT_LT0)\n",
    "    print(f\"Saved input tensor + global context: {INPUT_LT0}\")\n",
    "\n",
    "    # Save binary targets for each lead time\n",
    "    for i, core in enumerate(core_series):\n",
    "        target_tensor = torch.tensor(core != 0, dtype=torch.uint8)\n",
    "        output_file_path = OUTPUT_PATHS[f\"LT{lead_times[i]}\"]\n",
    "        torch.save(target_tensor, output_file_path)\n",
    "    print(f\"Saved {len(lead_times)} targets for {NOWCAST_ORIGIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d419c9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([424, 463])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36cc28ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.4498,  26.7059,  -8.5071,  -8.3356,  26.5354,  26.8062, -75.6667,\n",
       "        360.0000,   1.0000,  60.0000])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"/gws/nopw/j04/wiser_ewsa/mrakotomanga/Intercomparison/raw/inputs_t0/input-20240102_0100.pt\")['input_tensor'][100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4bcbd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6659e+01,  2.6910e+01, -1.6836e+01,  ...,  3.1500e+02,\n",
       "          1.0000e+00,  0.0000e+00],\n",
       "        [-1.6110e+01,  2.8518e+01, -1.6710e+01,  ...,  1.7280e+03,\n",
       "          1.0000e+00,  0.0000e+00],\n",
       "        [-1.5930e+01,  2.9491e+01, -1.6139e+01,  ...,  4.8600e+02,\n",
       "          1.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [-7.0615e+00,  3.0514e+01, -7.0615e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.2000e+02],\n",
       "        [-1.0389e+01,  3.0210e+01, -1.0389e+01,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.2000e+02],\n",
       "        [-9.7025e+00,  2.6013e+01, -9.7025e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  1.2000e+02]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
