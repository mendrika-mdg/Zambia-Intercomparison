{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d7b376",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8e4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np      \n",
    "from netCDF4 import Dataset  \n",
    "from scipy.ndimage import label\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.insert(1, \"/home/users/mendrika/SSA/SA/module\")\n",
    "import snflics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e679d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = 547, 970\n",
    "x_min, x_max = 1436, 1898\n",
    "\n",
    "CONTEXT_LAT_MIN = -19\n",
    "CONTEXT_LAT_MAX = -7\n",
    "CONTEXT_LON_MIN = 21\n",
    "CONTEXT_LON_MAX = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801662aa",
   "metadata": {},
   "source": [
    "# Subroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a37ab",
   "metadata": {},
   "source": [
    "## Read core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63b6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_core(file, y_min, y_max, x_min, x_max):\n",
    "\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"The file '{file}' does not exist.\")\n",
    "    try:\n",
    "        # using a context manager to ensure proper file closure\n",
    "        with Dataset(file, \"r\") as data:\n",
    "            cores = data.variables[\"cores\"][0, y_min:y_max+1, x_min:x_max+1]\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error opening NetCDF file: {file}. {e}\")\n",
    "\n",
    "    return cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f59d3",
   "metadata": {},
   "source": [
    "## Move a datetime by any given dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2548a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hour(date_dict, hours_to_add, minutes_to_add):\n",
    "    \"\"\"\n",
    "    Add hours and minutes to a datetime dictionary and return the updated dict and a generated file path.\n",
    "\n",
    "    Args:\n",
    "        date_dict     (dict): Keys: 'year', 'month', 'day', 'hour', 'minute' as strings, e.g. \"01\", \"23\"\n",
    "        hours_to_add   (int): Number of hours to add.\n",
    "        minutes_to_add (int): Number of minutes to add.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - dict: Updated datetime dictionary with all fields as zero-padded strings.\n",
    "            - str: File path in the format YYYY/MM/YYYYMMDDHHMM.nc\n",
    "    \"\"\"\n",
    "    # Parse the original time\n",
    "    time_obj = datetime(\n",
    "        int(date_dict[\"year\"]),\n",
    "        int(date_dict[\"month\"]),\n",
    "        int(date_dict[\"day\"]),\n",
    "        int(date_dict[\"hour\"]),\n",
    "        int(date_dict[\"minute\"])\n",
    "    )\n",
    "\n",
    "    # Add hours\n",
    "    updated = time_obj + timedelta(hours=hours_to_add, minutes=minutes_to_add)\n",
    "\n",
    "    # Format updated dictionary\n",
    "    new_date_dict = {\n",
    "        \"year\":   f\"{updated.year:04d}\",\n",
    "        \"month\":  f\"{updated.month:02d}\",\n",
    "        \"day\":    f\"{updated.day:02d}\",\n",
    "        \"hour\":   f\"{updated.hour:02d}\",\n",
    "        \"minute\": f\"{updated.minute:02d}\"\n",
    "    }\n",
    "\n",
    "    # Generate file path\n",
    "    file_path = f\"{new_date_dict['year']}/{new_date_dict['month']}/{new_date_dict['year']}{new_date_dict['month']}{new_date_dict['day']}{new_date_dict['hour']}{new_date_dict['minute']}.nc\"\n",
    "\n",
    "\n",
    "    return {'time': new_date_dict, 'path': file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300d668",
   "metadata": {},
   "source": [
    "## Extract a box of given size for y,x coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e3a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_box(matrix, y, x, box_size=3):\n",
    "    half = box_size // 2\n",
    "    y_min = max(y - half, 0)\n",
    "    y_max = min(y + half + 1, matrix.shape[0])\n",
    "    x_min = max(x - half, 0)\n",
    "    x_max = min(x + half + 1, matrix.shape[1])\n",
    "    return matrix[y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba877b00",
   "metadata": {},
   "source": [
    "## Creating storm databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storm_database(data_t, lats, lons):\n",
    "    \"\"\"\n",
    "    Identify storm cores and extract features for each core.\n",
    "\n",
    "    Args:\n",
    "        data_t (Dataset): Dataset containing 'cores' and 'tir' variables.\n",
    "        lats, lons (np.ndarray): 2D lat/lon arrays of the domain.\n",
    "\n",
    "    Returns:\n",
    "        dict: Storm database indexed by core label.\n",
    "    \"\"\"\n",
    "    \n",
    "    # From lat lon boxes\n",
    "\n",
    "    cores_t = data_t[\"cores\"][0, y_min:y_max+1, x_min:x_max+1]\n",
    "    tir_t   = data_t['tir'][0,   y_min:y_max+1, x_min:x_max+1]\n",
    "\n",
    "    Pmax_lat, Pmax_lon = data_t[\"max_lat\"][:], data_t[\"max_lon\"][:]\n",
    "\n",
    "    valid_indices = (\n",
    "                    (Pmax_lon >= CONTEXT_LON_MIN) & (Pmax_lon <= CONTEXT_LON_MAX) &\n",
    "                    (Pmax_lat >= CONTEXT_LAT_MIN) & (Pmax_lat <= CONTEXT_LAT_MAX)\n",
    "                )\n",
    "\n",
    "    Pmax_lat = Pmax_lat[valid_indices]\n",
    "    Pmax_lon = Pmax_lon[valid_indices]\n",
    "\n",
    "    # label all cores\n",
    "    labeled_array, _ = label(cores_t != 0)     \n",
    "    core_labels = np.unique(labeled_array[labeled_array != 0])\n",
    "\n",
    "    # creating database of sizes, intensities and ctts\n",
    "    dict_storm_size = {lab: np.sum(labeled_array == lab) * 9 for lab in core_labels}\n",
    "    dict_storm_intensity = {lab: np.mean(cores_t[labeled_array == lab]) for lab in core_labels}\n",
    "\n",
    "    # Compute min temperature of a core but based on 3x3 average around min TIR\n",
    "    dict_storm_temperature = {}\n",
    "\n",
    "    for lab in core_labels:\n",
    "\n",
    "        mask = (labeled_array == lab)\n",
    "        tir_core = tir_t[mask]      \n",
    "\n",
    "        # tir_core is a 1D array                   \n",
    "        min_index = np.argmin(tir_core)        \n",
    "\n",
    "        # Get absolute indices of the min location\n",
    "        yx_indices = np.argwhere(mask)[min_index]\n",
    "        y, x = yx_indices\n",
    "\n",
    "        # extract a 3x3 box around the location with min temperature\n",
    "        box = extract_box(tir_t, y, x)\n",
    "        avg_tir = float(np.mean(box))\n",
    "        dict_storm_temperature[lab] = avg_tir\n",
    "\n",
    "    storm_database = {}\n",
    "    for lat, lon in zip(Pmax_lat, Pmax_lon):\n",
    "        try:\n",
    "            y, x = snflics.to_yx(lat, lon, lats, lons)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        lab = labeled_array[y, x]\n",
    "        if lab == 0 or lab in storm_database:\n",
    "            continue\n",
    "        storm_database[int(lab)] = {\n",
    "            \"lat\": lat, \n",
    "            \"lon\": lon, \n",
    "            \"wp\": dict_storm_intensity[lab], \n",
    "            \"tir\": dict_storm_temperature[lab],\n",
    "            \"size\": dict_storm_size[lab], \n",
    "            \"mask\": 1\n",
    "        }\n",
    "    return storm_database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47bca889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fictional_storm(context_lat_min, context_lat_max, context_lon_min, context_lon_max, min_km_buffer=10, max_deg_buffer=4.5):\n",
    "    \"\"\"\n",
    "    Generate a synthetic storm outside context domain but near enough.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (storm_id, storm_dict)\n",
    "    \"\"\"\n",
    "    lat_range = (context_lat_min - max_deg_buffer, context_lat_max + max_deg_buffer)\n",
    "    lon_range = (context_lon_min - max_deg_buffer, context_lon_max + max_deg_buffer)\n",
    "    while True:\n",
    "        lat, lon = np.random.uniform(*lat_range), np.random.uniform(*lon_range)\n",
    "        if context_lat_min <= lat <= context_lat_max and context_lon_min <= lon <= context_lon_max:\n",
    "            continue\n",
    "        d_north = haversine_distance(lat, lon, context_lat_max, lon)\n",
    "        d_south = haversine_distance(lat, lon, context_lat_min, lon)\n",
    "        d_east  = haversine_distance(lat, lon, lat, context_lon_max)\n",
    "        d_west  = haversine_distance(lat, lon, lat, context_lon_min)\n",
    "        if min(d_north, d_south, d_east, d_west) < min_km_buffer:\n",
    "            continue\n",
    "\n",
    "        # lat lon in the buffer zone\n",
    "        # warm enough to be non-convective, realistic for Africa, covers both day and night\n",
    "        return ('artificial', {'lat': lat, 'lon': lon, 'wp': 0.0, 'tir': float(np.random.uniform(20.0, 35.0)), 'size': 0, 'mask': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67a0775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_observed_storms(storm_db, nb_x0, context_lat_min, context_lat_max, context_lon_min, context_lon_max):\n",
    "\n",
    "    # Convert dict to list of (key, value) tuples\n",
    "    storm_list = list(storm_db.items())\n",
    "\n",
    "    if len(storm_list) >= nb_x0:\n",
    "        # taking the nb_x0 strongest cores if there are more than the max number of cores allowed by the model\n",
    "        sorted_db = sorted(storm_list, key=lambda item: item[1]['tir'], reverse=False)\n",
    "        return sorted_db[:nb_x0]\n",
    "    else:\n",
    "        # apply padding when there are less cores observed at time t0\n",
    "        needed = nb_x0 - len(storm_list)\n",
    "        storm_list.extend([\n",
    "            generate_fictional_storm(\n",
    "                context_lat_min=context_lat_min, \n",
    "                context_lat_max=context_lat_max, \n",
    "                context_lon_min=context_lon_min,\n",
    "                context_lon_max=context_lon_max\n",
    "            ) \n",
    "            for _ in range(needed)\n",
    "        ])\n",
    "        return storm_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649fb57",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c9aa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From lat lon boxes\n",
    "y_min, y_max = 547, 970\n",
    "x_min, x_max = 1436, 1898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2127fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = np.load(\"/gws/nopw/j04/cocoon/SSA_domain/lat_lon_2268_2080.npz\")\n",
    "lons = geodata[\"lon\"][y_min:y_max+1, x_min:x_max+1]\n",
    "lats = geodata[\"lat\"][y_min:y_max+1, x_min:x_max+1]\n",
    "\n",
    "CONTEXT_LAT_MIN = -19\n",
    "CONTEXT_LAT_MAX = -7\n",
    "CONTEXT_LON_MIN = 21\n",
    "CONTEXT_LON_MAX = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "08e406aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_array(time_obs, data, time_lag):\n",
    "    \"\"\"\n",
    "    Transform list of (id, dict) into numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    year = int(time_obs['year'])\n",
    "    month = int(time_obs['month'])\n",
    "    day = int(time_obs['day'])\n",
    "    hour = int(time_obs['hour'])\n",
    "    minute = int(time_obs['minute'])\n",
    "    result = []\n",
    "    \n",
    "    for _, entry in data:\n",
    "        lat = float(entry['lat'])\n",
    "        lon = float(entry['lon'])\n",
    "        tir = float(entry['tir'])\n",
    "        size = int(entry['size'])\n",
    "        mask = int(entry['mask'])\n",
    "        result.append([year, month, day, hour, minute, lat, lon, tir, size, mask, time_lag])\n",
    "    \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4e86bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_t, time_t0, nb_x0, time_lag):\n",
    "    try:        \n",
    "        with Dataset(file_t, \"r\") as data_t:\n",
    "\n",
    "            x0_lat, x0_lon = data_t[\"max_lat\"][:], data_t[\"max_lon\"][:]\n",
    "            if x0_lat.size == 0 or x0_lon.size == 0:\n",
    "                return\n",
    "\n",
    "            storm_database = create_storm_database(data_t, lats, lons)\n",
    "\n",
    "            X_features = pad_observed_storms(storm_database, nb_x0,\n",
    "                                               CONTEXT_LAT_MIN, CONTEXT_LAT_MAX,\n",
    "                                               CONTEXT_LON_MIN, CONTEXT_LON_MAX)\n",
    "\n",
    "            input_features = transform_to_array(time_t0, X_features, time_lag)\n",
    "            input_tensor = torch.tensor(input_features, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {file_t}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f9a60b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2024\"\n",
    "\n",
    "# Data where all the historical cores are located and the output folder\n",
    "DATA_PATH = \"/gws/nopw/j04/cocoon/SSA_domain/ch9_wavelet/\"\n",
    "OUTPUT_FOLDER = \"/gws/nopw/j04/wiser_ewsa/mrakotomanga/Intercomparison/raw\"\n",
    "\n",
    "# year of interest\n",
    "all_files = [file for file in snflics.all_files_in(DATA_PATH) if snflics.get_time(file)[\"year\"] == YEAR and snflics.get_time(file)[\"month\"] in [\"12\", \"01\", \"02\"]]\n",
    "all_files.sort()\n",
    "\n",
    "# Number of storms to consider (after analysing the whole dataset)\n",
    "NB_X0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8557ad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5661"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "97ae0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 2.0240e+03,  1.0000e+00,  1.0000e+00,  2.0000e+00,  3.0000e+01,\n",
      "         -1.1565e+01,  3.2228e+01, -8.0444e+01,  9.0990e+03,  1.0000e+00,\n",
      "          0.0000e+00]]), tensor([[ 2.0240e+03,  1.0000e+00,  1.0000e+00,  2.0000e+00,  3.0000e+01,\n",
      "         -1.1653e+01,  3.2278e+01, -8.0556e+01,  2.6100e+03,  1.0000e+00,\n",
      "          6.0000e+01]]), tensor([[ 2.0240e+03,  1.0000e+00,  1.0000e+00,  2.0000e+00,  3.0000e+01,\n",
      "         -1.1536e+01,  3.2224e+01, -7.7444e+01,  1.4400e+02,  1.0000e+00,\n",
      "          1.2000e+02]])]\n"
     ]
    }
   ],
   "source": [
    "for file_t in all_files[10:11]:\n",
    "\n",
    "    # nowcast origin\n",
    "    time_t = snflics.get_time(file_t)\n",
    "\n",
    "    # lag times considered in minute\n",
    "    lag_before_t = [0, 60, 120]\n",
    "    file_before_t = [DATA_PATH + update_hour(time_t, hours_to_add=0, minutes_to_add=-m)[\"path\"] for m in lag_before_t]\n",
    "\n",
    "    # lead times considered in hour\n",
    "    lead_times = [0, 1, 2, 4, 6]\n",
    "    file_lead_times = [DATA_PATH + update_hour(time_t, hours_to_add=h, minutes_to_add=0)[\"path\"] for h in lead_times]     \n",
    "\n",
    "    # output folders\n",
    "    NOWCAST_ORIGIN = f\"{time_t['year']}{time_t['month']}{time_t['day']}_{time_t['hour']}{time_t['minute']}\"\n",
    "\n",
    "    INPUT_LT0 = f\"{OUTPUT_FOLDER}/inputs_t0/input-{NOWCAST_ORIGIN}.pt\"\n",
    "    OUTPUT_PATHS = {\n",
    "        f\"LT{i}\": f\"{OUTPUT_FOLDER}/targets_t{i}/target-{NOWCAST_ORIGIN}.pt\"\n",
    "        for i in lead_times\n",
    "    }\n",
    "\n",
    "    # If all past and forecast files exist\n",
    "    if all(os.path.exists(f) for f in file_lead_times) and all(os.path.exists(f) for f in file_before_t):\n",
    "        try:\n",
    "            core_series = [prepare_core(f) for f in file_lead_times]\n",
    "        except OSError:\n",
    "            continue\n",
    "\n",
    "        with Dataset(file_t, \"r\") as data_t:\n",
    "            Pmax_lat = data_t[\"max_lat\"][:]\n",
    "            Pmax_lon = data_t[\"max_lon\"][:]\n",
    "\n",
    "            valid_indices = (\n",
    "                    (Pmax_lon >= CONTEXT_LON_MIN) & (Pmax_lon <= CONTEXT_LON_MAX) &\n",
    "                    (Pmax_lat >= CONTEXT_LAT_MIN) & (Pmax_lat <= CONTEXT_LAT_MAX)\n",
    "                )\n",
    "\n",
    "            Pmax_lat = Pmax_lat[valid_indices]\n",
    "            Pmax_lon = Pmax_lon[valid_indices]\n",
    "\n",
    "            # only considers cores in the target domain\n",
    "\n",
    "            if Pmax_lat.size != 0:\n",
    "\n",
    "                input_tensor = []\n",
    "                for i, f in enumerate(file_before_t):\n",
    "                    input_tensor.append(process_file(f, time_t, NB_X0, lag_before_t[i]))\n",
    "                print(input_tensor)\n",
    "                input_tensor = torch.cat(input_tensor, dim=0)\n",
    "                torch.save(input_tensor, INPUT_LT0)\n",
    "\n",
    "                # Process targets for each lead time\n",
    "                for i, core in enumerate(core_series):\n",
    "                    target_tensor = torch.tensor(core != 0, dtype=torch.uint8)\n",
    "                    output_file_path = OUTPUT_PATHS[f\"LT{lead_times[i]}\"]\n",
    "                    torch.save(target_tensor, output_file_path)\n",
    "            else:\n",
    "                print(\"No core in the domain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
